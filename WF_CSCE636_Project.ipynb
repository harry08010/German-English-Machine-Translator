{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I1zQTwsqbAW9"
   },
   "source": [
    "# **Machine Translation - CSCE 636 Project**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "The goal of this project is to implement a machine translation model. In the first step, a dataset of German to English terms will be used to train a translation model. The dataset can be accessed here:\n",
    "\n",
    "[German-English deu-eng.zip](http://www.manythings.org/anki/deu-eng.zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CslsyjrebNzl"
   },
   "source": [
    "## **Load the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "01Me3Cxpbqgt"
   },
   "outputs": [],
   "source": [
    "# Unzip the data file\n",
    "import zipfile\n",
    "zip_ref = zipfile.ZipFile('deu-eng.zip', 'r')\n",
    "zip_ref.extractall('data-deu-eng')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rvfCQYlxd0Kp"
   },
   "source": [
    "The file called deu.txt that contains 152,820 pairs of English to German phases, one pair per line with a tab separating the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "u8HyAM_td54x",
    "outputId": "fbf22c27-cb70-4836-ff02-97043687a229"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi. Hallo! \n",
      "Hi. Grüß Gott! \n",
      "Run! Lauf! \n",
      "Wow! Potzdonner! \n",
      "Wow! Donnerwetter! \n"
     ]
    }
   ],
   "source": [
    "# First 5 lines of the deu.txt file\n",
    "import re\n",
    "with open(\"data-deu-eng/deu.txt\") as datafile:\n",
    "    head = [next(datafile) for x in range(5)]\n",
    "\n",
    "for pair in head:\n",
    "  print(re.sub('\\s+',' ',pair))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GU8toYpDmWXd"
   },
   "source": [
    "## **Prepare the Text Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SF2H1l3Mmcau"
   },
   "source": [
    "Reviewing the raw data, there are some observations:\n",
    "* Punctuation;\n",
    "* Uppercase and lowercase; \n",
    "* Special characters in German; \n",
    "* Same phrases in English with different translations in German.\n",
    "\n",
    "Therefore, it is necessary to clean the data. So the data preparation is divided into two subsections:\n",
    "**clean text and split text**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KA3SI2UyoRMh"
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "import string\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, mode='rt', encoding='utf-8') # preserving the Unicode German characters\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ITtyMtJ-n0m9"
   },
   "source": [
    "**1. Clean Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ims6_jcVofr2"
   },
   "outputs": [],
   "source": [
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "\tlines = doc.strip().split('\\n')\n",
    "\tpairs = [line.split('\\t') for line in  lines] # seperate the English and German phrases\n",
    "\treturn pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TcuEyEz-ozQ6"
   },
   "outputs": [],
   "source": [
    "# clean a list of lines\n",
    "def clean_pairs(lines):\n",
    "\tcleaned = list()\n",
    "  \n",
    "\t# prepare regex for char filtering\n",
    "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "\t# prepare translation table for removing punctuation\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\tfor pair in lines:\n",
    "\t\tclean_pair = list()\n",
    "\t\tfor line in pair:\n",
    "\t\t\tline = normalize('NFD', line).encode('ascii', 'ignore') # normalize unicode characters\n",
    "\t\t\tline = line.decode('UTF-8')\n",
    "\t\t\tline = line.split()\n",
    "\t\t\tline = [word.translate(table) for word in line]         # remove punctuation from each token\n",
    "\t\t\tline = [re_print.sub('', w) for w in line]              # remove non-printable chars form each token\n",
    "\t\t\tline = [word for word in line if word.isalpha()]        # remove tokens with numbers in them\n",
    "\t\t\tclean_pair.append(' '.join(line))\n",
    "\t\tcleaned.append(clean_pair)\n",
    "\treturn array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1734
    },
    "colab_type": "code",
    "id": "Mbh_Fw8Cp3dF",
    "outputId": "f5694b30-4c83-4cdf-d7a7-e24eb093786a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german.pkl\n",
      "[Hi] => [Hallo]\n",
      "[Hi] => [Gru Gott]\n",
      "[Run] => [Lauf]\n",
      "[Wow] => [Potzdonner]\n",
      "[Wow] => [Donnerwetter]\n",
      "[Fire] => [Feuer]\n",
      "[Help] => [Hilfe]\n",
      "[Help] => [Zu Hulf]\n",
      "[Stop] => [Stopp]\n",
      "[Wait] => [Warte]\n",
      "[Go on] => [Mach weiter]\n",
      "[Hello] => [Hallo]\n",
      "[I ran] => [Ich rannte]\n",
      "[I see] => [Ich verstehe]\n",
      "[I see] => [Aha]\n",
      "[I try] => [Ich probiere es]\n",
      "[I won] => [Ich hab gewonnen]\n",
      "[I won] => [Ich habe gewonnen]\n",
      "[Smile] => [Lacheln]\n",
      "[Cheers] => [Zum Wohl]\n",
      "[Freeze] => [Keine Bewegung]\n",
      "[Freeze] => [Stehenbleiben]\n",
      "[Got it] => [Kapiert]\n",
      "[Got it] => [Verstanden]\n",
      "[Got it] => [Einverstanden]\n",
      "[He ran] => [Er rannte]\n",
      "[He ran] => [Er lief]\n",
      "[Hop in] => [Mach mit]\n",
      "[Hug me] => [Druck mich]\n",
      "[Hug me] => [Nimm mich in den Arm]\n",
      "[Hug me] => [Umarme mich]\n",
      "[I fell] => [Ich fiel]\n",
      "[I fell] => [Ich fiel hin]\n",
      "[I fell] => [Ich sturzte]\n",
      "[I fell] => [Ich bin hingefallen]\n",
      "[I fell] => [Ich bin gesturzt]\n",
      "[I know] => [Ich wei]\n",
      "[I lied] => [Ich habe gelogen]\n",
      "[I lost] => [Ich habe verloren]\n",
      "[I paid] => [Ich habe bezahlt]\n",
      "[I paid] => [Ich zahlte]\n",
      "[I swim] => [Ich schwimme]\n",
      "[Im] => [Ich bin Jahre alt]\n",
      "[Im] => [Ich bin]\n",
      "[Im OK] => [Mir gehts gut]\n",
      "[Im OK] => [Es geht mir gut]\n",
      "[Im up] => [Ich bin wach]\n",
      "[Im up] => [Ich bin auf]\n",
      "[No way] => [Unmoglich]\n",
      "[No way] => [Das kommt nicht in Frage]\n",
      "[No way] => [Das gibts doch nicht]\n",
      "[No way] => [Ausgeschlossen]\n",
      "[No way] => [In keinster Weise]\n",
      "[Really] => [Wirklich]\n",
      "[Really] => [Echt]\n",
      "[Really] => [Im Ernst]\n",
      "[Thanks] => [Danke]\n",
      "[Try it] => [Versuchs]\n",
      "[Why me] => [Warum ich]\n",
      "[Ask Tom] => [Frag Tom]\n",
      "[Ask Tom] => [Fragen Sie Tom]\n",
      "[Ask Tom] => [Fragt Tom]\n",
      "[Be cool] => [Entspann dich]\n",
      "[Be fair] => [Sei nicht ungerecht]\n",
      "[Be fair] => [Sei fair]\n",
      "[Be nice] => [Sei nett]\n",
      "[Be nice] => [Seien Sie nett]\n",
      "[Beat it] => [Geh weg]\n",
      "[Beat it] => [Hau ab]\n",
      "[Beat it] => [Verschwinde]\n",
      "[Beat it] => [Verdufte]\n",
      "[Beat it] => [Mach dich fort]\n",
      "[Beat it] => [Zieh Leine]\n",
      "[Beat it] => [Mach dich vom Acker]\n",
      "[Beat it] => [Verzieh dich]\n",
      "[Beat it] => [Verkrumele dich]\n",
      "[Beat it] => [Troll dich]\n",
      "[Beat it] => [Zisch ab]\n",
      "[Beat it] => [Pack dich]\n",
      "[Beat it] => [Mach ne Fliege]\n",
      "[Beat it] => [Schwirr ab]\n",
      "[Beat it] => [Mach die Sause]\n",
      "[Beat it] => [Scher dich weg]\n",
      "[Beat it] => [Scher dich fort]\n",
      "[Call me] => [Ruf mich an]\n",
      "[Come in] => [Komm herein]\n",
      "[Come in] => [Herein]\n",
      "[Come on] => [Komm]\n",
      "[Come on] => [Kommt]\n",
      "[Come on] => [Mach schon]\n",
      "[Come on] => [Macht schon]\n",
      "[Come on] => [Komm schon]\n",
      "[Get Tom] => [Hol Tom]\n",
      "[Get out] => [Raus]\n",
      "[Get out] => [Geh raus]\n",
      "[Get out] => [Geht raus]\n",
      "[Go away] => [Geh weg]\n",
      "[Go away] => [Hau ab]\n",
      "[Go away] => [Verschwinde]\n",
      "[Go away] => [Verdufte]\n"
     ]
    }
   ],
   "source": [
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "\tdump(sentences, open(filename, 'wb'))\n",
    "\tprint('Saved: %s' % filename)\n",
    "\n",
    "# load dataset\n",
    "filename = 'data-deu-eng/deu.txt'\n",
    "doc = load_doc(filename)\n",
    "pairs = to_pairs(doc)\n",
    "clean_pairs = clean_pairs(pairs)\n",
    "\n",
    "# save clean pairs to file\n",
    "save_clean_data(clean_pairs, 'english-german.pkl')\n",
    "\n",
    "# spot check\n",
    "for i in range(100):\n",
    "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xc373GPvqx7t"
   },
   "source": [
    "**2. Split Text**\n",
    "\n",
    "The cleaned data now has over 150,000 phrase pairs. Here, the first 50,000 examples will be used - the first 45,000 examples for training and the remaining 5000 for testing *(To reduce the training time, originally, only the first 10,000 examples will be used - the first 9,000 will be used for training and the remaining 1,000 examples for testing. )*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "gNiFIH06rloL",
    "outputId": "f3aa33b0-ab57-403e-ea32-b86d85ef7212"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german-both.pkl\n",
      "Saved: english-german-train.pkl\n",
      "Saved: english-german-test.pkl\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from numpy.random import rand\n",
    "from numpy.random import shuffle\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "\tdump(sentences, open(filename, 'wb'))\n",
    "\tprint('Saved: %s' % filename)\n",
    "\n",
    "raw_dataset = load_clean_sentences('english-german.pkl')\n",
    "\n",
    "# reduce dataset size\n",
    "n_sentences = 50000\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "\n",
    "# random shuffle\n",
    "shuffle(dataset)\n",
    "shuffle(dataset)\n",
    "\n",
    "# split into train/test\n",
    "train, test = dataset[:48000], dataset[48000:]\n",
    "\n",
    "# save training/testing data\n",
    "save_clean_data(dataset, 'english-german-both.pkl')\n",
    "save_clean_data(train, 'english-german-train.pkl')\n",
    "save_clean_data(test, 'english-german-test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GBDXGdgKyRYc"
   },
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Activation\n",
    "from attention_decoder import AttentionDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    "\n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "\treturn max(len(line.split()) for line in lines)\n",
    "\n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "\t# integer encode sequences\n",
    "\tX = tokenizer.texts_to_sequences(lines)\n",
    "  \n",
    "\t# pad sequences with 0 values\n",
    "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
    "  \n",
    "\treturn X\n",
    "\n",
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "\tylist = list()\n",
    "\tfor sequence in sequences:\n",
    "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "\t\tylist.append(encoded)\n",
    "\ty = array(ylist)\n",
    "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "\treturn y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 6366\n",
      "English Max Length: 7\n",
      "German Vocabulary Size: 10752\n",
      "German Max Length: 17\n"
     ]
    }
   ],
   "source": [
    "# load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')\n",
    "\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))\n",
    "\n",
    "# prepare training data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "\n",
    "# prepare validation data\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2009
    },
    "colab_type": "code",
    "id": "u42mG-QVyjn3",
    "outputId": "bc48fdb5-abfc-463d-94ad-e3067b4e3660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 3778\n",
      "English Max Length: 6\n",
      "German Vocabulary Size: 5830\n",
      "German Max Length: 10\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 10, 256)           1492480   \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_7 (RepeatVecto (None, 6, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 6, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 6, 3778)           970946    \n",
      "=================================================================\n",
      "Total params: 3,514,050\n",
      "Trainable params: 3,514,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/30\n",
      " - 51s - loss: 3.7720 - acc: 0.4877 - val_loss: 3.2268 - val_acc: 0.5028\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.22677, saving model to model.h5\n",
      "Epoch 2/30\n",
      " - 33s - loss: 3.1157 - acc: 0.5141 - val_loss: 3.0779 - val_acc: 0.5248\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.22677 to 3.07788, saving model to model.h5\n",
      "Epoch 3/30\n",
      " - 33s - loss: 2.8851 - acc: 0.5381 - val_loss: 2.8278 - val_acc: 0.5506\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.07788 to 2.82776, saving model to model.h5\n",
      "Epoch 4/30\n",
      " - 33s - loss: 2.6198 - acc: 0.5673 - val_loss: 2.6496 - val_acc: 0.5735\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.82776 to 2.64961, saving model to model.h5\n",
      "Epoch 5/30\n",
      " - 32s - loss: 2.4170 - acc: 0.5881 - val_loss: 2.4901 - val_acc: 0.5938\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.64961 to 2.49008, saving model to model.h5\n",
      "Epoch 6/30\n",
      " - 33s - loss: 2.2181 - acc: 0.6134 - val_loss: 2.3541 - val_acc: 0.6173\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.49008 to 2.35411, saving model to model.h5\n",
      "Epoch 7/30\n",
      " - 32s - loss: 2.0320 - acc: 0.6377 - val_loss: 2.2223 - val_acc: 0.6359\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.35411 to 2.22229, saving model to model.h5\n",
      "Epoch 8/30\n",
      " - 33s - loss: 1.8589 - acc: 0.6597 - val_loss: 2.1134 - val_acc: 0.6474\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.22229 to 2.11340, saving model to model.h5\n",
      "Epoch 9/30\n",
      " - 33s - loss: 1.7014 - acc: 0.6789 - val_loss: 2.0252 - val_acc: 0.6628\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.11340 to 2.02520, saving model to model.h5\n",
      "Epoch 10/30\n",
      " - 33s - loss: 1.5551 - acc: 0.6949 - val_loss: 1.9539 - val_acc: 0.6692\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.02520 to 1.95394, saving model to model.h5\n",
      "Epoch 11/30\n",
      " - 33s - loss: 1.4168 - acc: 0.7111 - val_loss: 1.8827 - val_acc: 0.6778\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.95394 to 1.88268, saving model to model.h5\n",
      "Epoch 12/30\n",
      " - 32s - loss: 1.2887 - acc: 0.7280 - val_loss: 1.8319 - val_acc: 0.6843\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.88268 to 1.83186, saving model to model.h5\n",
      "Epoch 13/30\n",
      " - 33s - loss: 1.1698 - acc: 0.7464 - val_loss: 1.7849 - val_acc: 0.6942\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.83186 to 1.78490, saving model to model.h5\n",
      "Epoch 14/30\n",
      " - 32s - loss: 1.0590 - acc: 0.7635 - val_loss: 1.7345 - val_acc: 0.6980\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.78490 to 1.73454, saving model to model.h5\n",
      "Epoch 15/30\n",
      " - 33s - loss: 0.9566 - acc: 0.7818 - val_loss: 1.7070 - val_acc: 0.7063\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.73454 to 1.70696, saving model to model.h5\n",
      "Epoch 16/30\n",
      " - 33s - loss: 0.8634 - acc: 0.8003 - val_loss: 1.6680 - val_acc: 0.7100\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.70696 to 1.66800, saving model to model.h5\n",
      "Epoch 17/30\n",
      " - 33s - loss: 0.7766 - acc: 0.8180 - val_loss: 1.6421 - val_acc: 0.7145\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.66800 to 1.64211, saving model to model.h5\n",
      "Epoch 18/30\n",
      " - 32s - loss: 0.6992 - acc: 0.8353 - val_loss: 1.6231 - val_acc: 0.7219\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.64211 to 1.62310, saving model to model.h5\n",
      "Epoch 19/30\n",
      " - 33s - loss: 0.6300 - acc: 0.8497 - val_loss: 1.6176 - val_acc: 0.7230\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.62310 to 1.61760, saving model to model.h5\n",
      "Epoch 20/30\n",
      " - 33s - loss: 0.5648 - acc: 0.8656 - val_loss: 1.6076 - val_acc: 0.7266\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.61760 to 1.60758, saving model to model.h5\n",
      "Epoch 21/30\n",
      " - 33s - loss: 0.5107 - acc: 0.8777 - val_loss: 1.5950 - val_acc: 0.7282\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.60758 to 1.59504, saving model to model.h5\n",
      "Epoch 22/30\n",
      " - 33s - loss: 0.4579 - acc: 0.8890 - val_loss: 1.5973 - val_acc: 0.7298\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.59504\n",
      "Epoch 23/30\n",
      " - 33s - loss: 0.4152 - acc: 0.8990 - val_loss: 1.5949 - val_acc: 0.7337\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.59504 to 1.59493, saving model to model.h5\n",
      "Epoch 24/30\n",
      " - 33s - loss: 0.3761 - acc: 0.9079 - val_loss: 1.5994 - val_acc: 0.7329\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.59493\n",
      "Epoch 25/30\n",
      " - 33s - loss: 0.3439 - acc: 0.9143 - val_loss: 1.5976 - val_acc: 0.7351\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.59493\n",
      "Epoch 26/30\n",
      " - 33s - loss: 0.3109 - acc: 0.9226 - val_loss: 1.6106 - val_acc: 0.7352\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.59493\n",
      "Epoch 27/30\n",
      " - 33s - loss: 0.2850 - acc: 0.9273 - val_loss: 1.6280 - val_acc: 0.7362\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.59493\n",
      "Epoch 28/30\n",
      " - 33s - loss: 0.2622 - acc: 0.9314 - val_loss: 1.6196 - val_acc: 0.7382\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.59493\n",
      "Epoch 29/30\n",
      " - 33s - loss: 0.2417 - acc: 0.9362 - val_loss: 1.6217 - val_acc: 0.7388\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.59493\n",
      "Epoch 30/30\n",
      " - 33s - loss: 0.2234 - acc: 0.9393 - val_loss: 1.6328 - val_acc: 0.7378\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.59493\n"
     ]
    }
   ],
   "source": [
    "# define baseline NMT model\n",
    "def define_baseline_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "  model.add(LSTM(n_units))\n",
    "#   model.add(AttentionL(src_timesteps))\n",
    "  model.add(RepeatVector(tar_timesteps))\n",
    "  model.add(LSTM(n_units, return_sequences=True))\n",
    "  model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "  return model\n",
    "\n",
    "            \n",
    "# define attention model\n",
    "def define_attention_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "  model.add(LSTM(n_units, return_sequences=True))\n",
    "  model.add(AttentionDecoder(n_units, tar_vocab))\n",
    "  return model\n",
    "\n",
    "# define model\n",
    "model = define_baseline_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# summarize defined model\n",
    "print(model.summary())\n",
    "plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "# fit model\n",
    "filename = 'model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "history = model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define bidirectional model\n",
    "def define_bidirectional_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "  model.add(Bidirectional(LSTM(n_units)))\n",
    "#   model.add(AttentionL(src_timesteps))\n",
    "  model.add(RepeatVector(tar_timesteps))\n",
    "  model.add(Bidirectional(LSTM(n_units, return_sequences=True)))\n",
    "  model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 6366\n",
      "English Max Length: 7\n",
      "German Vocabulary Size: 10752\n",
      "German Max Length: 17\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 17, 256)           2752512   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 512)               1050624   \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 7, 512)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 7, 512)            1574912   \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 7, 6366)           3265758   \n",
      "=================================================================\n",
      "Total params: 8,643,806\n",
      "Trainable params: 8,643,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 48000 samples, validate on 2000 samples\n",
      "Epoch 1/15\n",
      " - 247s - loss: 3.2737 - acc: 0.5156 - val_loss: 2.6482 - val_acc: 0.5769\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.64825, saving model to bi_model.h5\n",
      "Epoch 2/15\n",
      " - 240s - loss: 2.1888 - acc: 0.6255 - val_loss: 1.9562 - val_acc: 0.6511\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.64825 to 1.95620, saving model to bi_model.h5\n",
      "Epoch 3/15\n",
      " - 241s - loss: 1.5671 - acc: 0.6896 - val_loss: 1.6074 - val_acc: 0.6907\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.95620 to 1.60744, saving model to bi_model.h5\n",
      "Epoch 4/15\n",
      " - 239s - loss: 1.1559 - acc: 0.7414 - val_loss: 1.4270 - val_acc: 0.7166\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.60744 to 1.42703, saving model to bi_model.h5\n",
      "Epoch 5/15\n",
      " - 241s - loss: 0.8784 - acc: 0.7850 - val_loss: 1.3306 - val_acc: 0.7336\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.42703 to 1.33055, saving model to bi_model.h5\n",
      "Epoch 6/15\n",
      " - 241s - loss: 0.6858 - acc: 0.8221 - val_loss: 1.2876 - val_acc: 0.7433\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.33055 to 1.28757, saving model to bi_model.h5\n",
      "Epoch 7/15\n",
      " - 241s - loss: 0.5490 - acc: 0.8517 - val_loss: 1.2596 - val_acc: 0.7522\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.28757 to 1.25964, saving model to bi_model.h5\n",
      "Epoch 8/15\n",
      " - 241s - loss: 0.4525 - acc: 0.8744 - val_loss: 1.2744 - val_acc: 0.7546\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.25964\n",
      "Epoch 9/15\n",
      " - 241s - loss: 0.3806 - acc: 0.8912 - val_loss: 1.2746 - val_acc: 0.7586\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.25964\n",
      "Epoch 10/15\n",
      " - 239s - loss: 0.3231 - acc: 0.9063 - val_loss: 1.3014 - val_acc: 0.7557\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.25964\n",
      "Epoch 11/15\n",
      " - 240s - loss: 0.2806 - acc: 0.9170 - val_loss: 1.3281 - val_acc: 0.7600\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.25964\n",
      "Epoch 12/15\n",
      " - 241s - loss: 0.2491 - acc: 0.9257 - val_loss: 1.3326 - val_acc: 0.7584\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.25964\n",
      "Epoch 13/15\n",
      " - 241s - loss: 0.2250 - acc: 0.9326 - val_loss: 1.3571 - val_acc: 0.7606\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.25964\n",
      "Epoch 14/15\n",
      " - 241s - loss: 0.2067 - acc: 0.9376 - val_loss: 1.3759 - val_acc: 0.7593\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.25964\n",
      "Epoch 15/15\n",
      " - 241s - loss: 0.1930 - acc: 0.9413 - val_loss: 1.3995 - val_acc: 0.7590\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.25964\n"
     ]
    }
   ],
   "source": [
    "# load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')\n",
    "\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))\n",
    "\n",
    "bi_trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "bi_trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "bi_trainY = encode_output(bi_trainY, eng_vocab_size)\n",
    "bi_testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "bi_testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "bi_testY = encode_output(bi_testY, eng_vocab_size)\n",
    "\n",
    "bi_model = define_bidirectional_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
    "bi_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# summarize defined model\n",
    "print(bi_model.summary())\n",
    "plot_model(bi_model, to_file='bi_model.png', show_shapes=True)\n",
    "\n",
    "# fit model\n",
    "filename = 'bi_model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "history = bi_model.fit(bi_trainX, bi_trainY, epochs=15, batch_size=64, validation_data=(bi_testX, bi_testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 3778\n",
      "English Max Length: 6\n",
      "German Vocabulary Size: 5830\n",
      "German Max Length: 10\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 10, 256)           1492480   \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 10, 256)           525312    \n",
      "_________________________________________________________________\n",
      "AttentionDecoder (AttentionD (None, 10, 3778)          19704006  \n",
      "=================================================================\n",
      "Total params: 21,721,798\n",
      "Trainable params: 21,721,798\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 18000 samples, validate on 2000 samples\n",
      "Epoch 1/15\n",
      " - 295s - loss: 4.8913 - acc: 0.2823 - val_loss: 4.0609 - val_acc: 0.3495\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.06091, saving model to attention_model.h5\n",
      "Epoch 2/15\n",
      " - 272s - loss: 3.6314 - acc: 0.3974 - val_loss: 3.4086 - val_acc: 0.4417\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.06091 to 3.40856, saving model to attention_model.h5\n",
      "Epoch 3/15\n",
      " - 270s - loss: 2.9302 - acc: 0.4759 - val_loss: 3.0173 - val_acc: 0.4841\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.40856 to 3.01734, saving model to attention_model.h5\n",
      "Epoch 4/15\n",
      " - 271s - loss: 2.3906 - acc: 0.5341 - val_loss: 2.7286 - val_acc: 0.5249\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.01734 to 2.72859, saving model to attention_model.h5\n",
      "Epoch 5/15\n",
      " - 271s - loss: 1.9365 - acc: 0.5869 - val_loss: 2.5300 - val_acc: 0.5531\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.72859 to 2.53003, saving model to attention_model.h5\n",
      "Epoch 6/15\n",
      " - 271s - loss: 1.5605 - acc: 0.6426 - val_loss: 2.3888 - val_acc: 0.5754\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.53003 to 2.38880, saving model to attention_model.h5\n",
      "Epoch 7/15\n",
      " - 271s - loss: 1.2575 - acc: 0.6955 - val_loss: 2.2812 - val_acc: 0.5942\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.38880 to 2.28120, saving model to attention_model.h5\n",
      "Epoch 8/15\n",
      " - 270s - loss: 1.0152 - acc: 0.7427 - val_loss: 2.2193 - val_acc: 0.6064\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.28120 to 2.21928, saving model to attention_model.h5\n",
      "Epoch 9/15\n",
      " - 270s - loss: 0.8252 - acc: 0.7862 - val_loss: 2.1818 - val_acc: 0.6129\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.21928 to 2.18183, saving model to attention_model.h5\n",
      "Epoch 10/15\n",
      " - 283s - loss: 0.6774 - acc: 0.8200 - val_loss: 2.1726 - val_acc: 0.6223\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.18183 to 2.17256, saving model to attention_model.h5\n",
      "Epoch 11/15\n",
      " - 290s - loss: 0.5692 - acc: 0.8471 - val_loss: 2.1678 - val_acc: 0.6223\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.17256 to 2.16779, saving model to attention_model.h5\n",
      "Epoch 12/15\n",
      " - 275s - loss: 0.4860 - acc: 0.8655 - val_loss: 2.1662 - val_acc: 0.6281\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.16779 to 2.16618, saving model to attention_model.h5\n",
      "Epoch 13/15\n",
      " - 272s - loss: 0.4204 - acc: 0.8834 - val_loss: 2.1780 - val_acc: 0.6297\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.16618\n",
      "Epoch 14/15\n",
      " - 281s - loss: 0.3740 - acc: 0.8935 - val_loss: 2.1805 - val_acc: 0.6285\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.16618\n",
      "Epoch 15/15\n",
      " - 286s - loss: 0.3345 - acc: 0.9031 - val_loss: 2.2331 - val_acc: 0.6262\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.16618\n"
     ]
    }
   ],
   "source": [
    "# load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')\n",
    "\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))\n",
    "\n",
    "attention_trainX = encode_sequences(ger_tokenizer, max(ger_length, eng_length), train[:, 1])\n",
    "attention_trainY = encode_sequences(eng_tokenizer, max(ger_length, eng_length), train[:, 0])\n",
    "attention_trainY = encode_output(attention_trainY, eng_vocab_size)\n",
    "attention_testX = encode_sequences(ger_tokenizer, max(ger_length, eng_length), test[:, 1])\n",
    "attention_testY = encode_sequences(eng_tokenizer, max(ger_length, eng_length), test[:, 0])\n",
    "attention_testY = encode_output(attention_testY, eng_vocab_size)\n",
    "\n",
    "attention_model = define_attention_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
    "attention_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# summarize defined model\n",
    "print(attention_model.summary())\n",
    "plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "# fit model\n",
    "filename = 'attention_model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "history = attention_model.fit(attention_trainX, attention_trainY, epochs=15, batch_size=64, validation_data=(attention_testX, attention_testY), callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "enQAO33JNdah",
    "outputId": "8eba5331-24b8-4895-9ac1-51f898b4db2a"
   },
   "outputs": [],
   "source": [
    "print(train[0])\n",
    "print(trainX[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000\n"
     ]
    }
   ],
   "source": [
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4dfiZt1oQQ8t",
    "outputId": "0c0796a7-2825-4808-ba86-6236aee5f901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the money disappeared' 'das geld verschwand']\n",
      "[   4  119 2938    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(train[1])\n",
    "print(trainX[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ou94F5LLzG41"
   },
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fTO7C1gFzXhS"
   },
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n-dGUQGSzq9P"
   },
   "outputs": [],
   "source": [
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == integer:\n",
    "\t\t\treturn word\n",
    "\treturn None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oC56ziVqztlj"
   },
   "outputs": [],
   "source": [
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "\tprediction = model.predict(source, verbose=0)[0]\n",
    "\tintegers = [argmax(vector) for vector in prediction]\n",
    "\ttarget = list()\n",
    "\tfor i in integers:\n",
    "\t\tword = word_for_id(i, tokenizer)\n",
    "\t\tif word is None:\n",
    "\t\t\tbreak\n",
    "\t\ttarget.append(word)\n",
    "\treturn ' '.join(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gm8veHBhzwL6"
   },
   "outputs": [],
   "source": [
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "  smoothie = SmoothingFunction().method4\n",
    "  actual, predicted = list(), list()\n",
    "  correct_num = 0\n",
    "  total = 0\n",
    "  for i, source in enumerate(sources):\n",
    "    # translate encoded source text\n",
    "    source = source.reshape((1, source.shape[0]))\n",
    "    translation = predict_sequence(model, eng_tokenizer, source)\n",
    "    raw_target, raw_src = raw_dataset[i]\n",
    "    total += 1\n",
    "    if translation == raw_target.lower():\n",
    "      correct_num += 1\n",
    "    if i < 10:\n",
    "      print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "    actual.append(raw_target.split())\n",
    "    predicted.append(translation.split())\n",
    "\t# calculate BLEU score\n",
    "  print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0), smoothing_function=smoothie))\n",
    "  print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie))\n",
    "  print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0), smoothing_function=smoothie))\n",
    "  print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie))\n",
    "  # calculate accuracy\n",
    "  print(float(correct_num / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "EXv2hWNszw-O",
    "outputId": "c615630d-ef33-4d72-8982-37b108f682c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "src=[ich habe es kaputtgemacht], target=[i broke it], predicted=[i broke it]\n",
      "src=[sie konnte ihn nicht leiden], target=[she hated him], predicted=[she hates him]\n",
      "src=[tom brachte mary um], target=[tom killed mary], predicted=[tom killed mary]\n",
      "src=[du hast ihm weh getan], target=[you hurt him], predicted=[you made him]\n",
      "src=[guckst du], target=[are you looking], predicted=[are you looking]\n",
      "src=[ist dein name tom], target=[is your name tom], predicted=[is your name tom]\n",
      "src=[ich kann tennis spielen], target=[i can play tennis], predicted=[i can play tennis]\n",
      "src=[sie haben es verdient], target=[they deserved it], predicted=[they deserved it]\n",
      "src=[sie kundigten], target=[they canceled], predicted=[they canceled]\n",
      "src=[dieser eimer ist undicht], target=[this bucket leaks], predicted=[this bucket leaks]\n",
      "BLEU-1: 0.069184\n",
      "BLEU-2: 0.215754\n",
      "BLEU-3: 0.301818\n",
      "BLEU-4: 0.269355\n",
      "0.6631666666666667\n",
      "test\n",
      "src=[er ist dj], target=[he is a dj], predicted=[hes a dj]\n",
      "src=[sie bezwang ihn], target=[she defeated him], predicted=[she forgave him]\n",
      "src=[tom wird schieen], target=[tom will shoot], predicted=[tomll quit]\n",
      "src=[tom lehnte sich zuruck], target=[tom leaned back], predicted=[tom walked back]\n",
      "src=[ich bin keine heilige], target=[im no saint], predicted=[im not a]\n",
      "src=[wir haben tom ausgetrickst], target=[we tricked tom], predicted=[we found tom]\n",
      "src=[ich schaue zu], target=[im watching], predicted=[im watching]\n",
      "src=[er hat kurze haare], target=[he has short hair], predicted=[he has short hair]\n",
      "src=[die kennen mich], target=[they know me], predicted=[they know me]\n",
      "src=[lass uns anfangen], target=[lets begin], predicted=[lets start]\n",
      "BLEU-1: 0.070122\n",
      "BLEU-2: 0.208303\n",
      "BLEU-3: 0.290631\n",
      "BLEU-4: 0.258558\n",
      "0.177\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = load_model('model.h5')\n",
    "# test on some training sequences\n",
    "print('train')\n",
    "evaluate_model(model, eng_tokenizer, trainX, train)\n",
    "# test on some test sequences\n",
    "print('test')\n",
    "evaluate_model(model, eng_tokenizer, testX, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "src=[Ich habe einen Camion], target=[I have a truck], predicted=[i have a truck]\n",
      "src=[Willst du nicht ins Bett], target=[Arent you going to bed], predicted=[dont you going to bed]\n",
      "src=[Er ist kein gewohnlicher Mann], target=[He is no ordinary man], predicted=[he is no ordinary man]\n",
      "src=[Ich kann Tom nicht einfach mit Nichtbeachtung strafen], target=[I cant just ignore Tom], predicted=[i cant just ignore tom]\n",
      "src=[Wir duschten], target=[We took showers], predicted=[we took showers]\n",
      "src=[Ich kann Montage nicht ausstehen], target=[I hate Mondays], predicted=[i hate mondays]\n",
      "src=[Was haben sie gesagt], target=[What did they say], predicted=[what did you say]\n",
      "src=[Bitte halten Sie mich auf dem Laufenden], target=[Please keep me updated], predicted=[please call me updated]\n",
      "src=[Beschleunigen wir das etwas], target=[Lets hurry it up], predicted=[lets on it]\n",
      "src=[Tom jubelte], target=[Tom was cheering], predicted=[tom cheered]\n",
      "BLEU-1: 0.051148\n",
      "BLEU-2: 0.190325\n",
      "BLEU-3: 0.283781\n",
      "BLEU-4: 0.257573\n",
      "0.5321875\n",
      "test\n",
      "src=[Es reut uns das getan zu haben], target=[We regret doing that], predicted=[we regret that that]\n",
      "src=[Mit wem sprichst du], target=[Who are you talking to], predicted=[who are you talking to]\n",
      "src=[Ich kann ihr nicht verzeihen], target=[I cant forgive her], predicted=[i cant forgive you]\n",
      "src=[Ich brauche Hilfe], target=[Im going to need help], predicted=[i need need help]\n",
      "src=[Ich wurde nicht angeheuert], target=[I wasnt hired], predicted=[i wouldnt not]\n",
      "src=[Bleib schon von ihr fern], target=[You stay away from her], predicted=[keep on from on]\n",
      "src=[Hast du etwas gesagt], target=[Did you say something], predicted=[did you say something]\n",
      "src=[Liebe existiert nicht], target=[Love doesnt exist], predicted=[doesnt doesnt love]\n",
      "src=[Wir posierten fur Fotos], target=[We posed for pictures], predicted=[we in a new]\n",
      "src=[Kennt Tom Maria], target=[Does Tom know Mary], predicted=[does tom know mary]\n"
     ]
    }
   ],
   "source": [
    "# load bidirectional model\n",
    "bi_model = load_model('bi_model.h5')\n",
    "print('train')\n",
    "evaluate_model(bi_model, eng_tokenizer, bi_trainX, train)\n",
    "# test on some test sequences\n",
    "print('test')\n",
    "evaluate_model(bi_model, eng_tokenizer, bi_testX, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "src=[ich habe es kaputtgemacht], target=[i broke it], predicted=[i broke it]\n",
      "src=[sie konnte ihn nicht leiden], target=[she hated him], predicted=[she hated him]\n",
      "src=[tom brachte mary um], target=[tom killed mary], predicted=[tom killed mary]\n",
      "src=[du hast ihm weh getan], target=[you hurt him], predicted=[you hurt him]\n",
      "src=[guckst du], target=[are you looking], predicted=[are you bored]\n",
      "src=[ist dein name tom], target=[is your name tom], predicted=[is your name tom]\n",
      "src=[ich kann tennis spielen], target=[i can play tennis], predicted=[i can play tennis]\n",
      "src=[sie haben es verdient], target=[they deserved it], predicted=[they deserved it]\n",
      "src=[sie kundigten], target=[they canceled], predicted=[they canceled]\n",
      "src=[dieser eimer ist undicht], target=[this bucket leaks], predicted=[this bucket leaks]\n",
      "BLEU-1: 0.066425\n",
      "BLEU-2: 0.213713\n",
      "BLEU-3: 0.302634\n",
      "BLEU-4: 0.270919\n",
      "0.6168333333333333\n",
      "test\n",
      "src=[er ist dj], target=[he is a dj], predicted=[hes a dj]\n",
      "src=[sie bezwang ihn], target=[she defeated him], predicted=[she despises him]\n",
      "src=[tom wird schieen], target=[tom will shoot], predicted=[tom will scream]\n",
      "src=[tom lehnte sich zuruck], target=[tom leaned back], predicted=[tom glanced back]\n",
      "src=[ich bin keine heilige], target=[im no saint], predicted=[im not a]\n",
      "src=[wir haben tom ausgetrickst], target=[we tricked tom], predicted=[we outsmarted tom]\n",
      "src=[ich schaue zu], target=[im watching], predicted=[im watching]\n",
      "src=[er hat kurze haare], target=[he has short hair], predicted=[he has long hair]\n",
      "src=[die kennen mich], target=[they know me], predicted=[they know me]\n",
      "src=[lass uns anfangen], target=[lets begin], predicted=[lets start]\n",
      "BLEU-1: 0.067954\n",
      "BLEU-2: 0.207223\n",
      "BLEU-3: 0.292060\n",
      "BLEU-4: 0.260501\n",
      "0.1755\n"
     ]
    }
   ],
   "source": [
    "# load attention model\n",
    "attention_model = load_model('attention_model.h5', custom_objects={'AttentionDecoder':AttentionDecoder})\n",
    "print('train')\n",
    "evaluate_model(attention_model, eng_tokenizer, attention_trainX, train)\n",
    "# test on some test sequences\n",
    "print('test')\n",
    "evaluate_model(attention_model, eng_tokenizer, attention_testX, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jW2DhSQO9bsy"
   },
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open('ger_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(ger_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('eng_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(eng_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "WF_CSCE636_Project.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
